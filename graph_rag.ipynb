{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG: Graph-Enhanced Retrieval-Augmented Generation\n",
    "\n",
    "## Overview\n",
    "\n",
    "GraphRAG is an advanced question-answering system that combines the power of graph-based knowledge representation with retrieval-augmented generation. It processes input documents to create a rich knowledge graph, which is then used to enhance the retrieval and generation of answers to user queries. The system leverages natural language processing, machine learning, and graph theory to provide more accurate and contextually relevant responses.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Traditional retrieval-augmented generation systems often struggle with maintaining context over long documents and making connections between related pieces of information. GraphRAG addresses these limitations by:\n",
    "\n",
    "1. Representing knowledge as an interconnected graph, allowing for better preservation of relationships between concepts.\n",
    "2. Enabling more intelligent traversal of information during the query process.\n",
    "3. Providing a visual representation of how information is connected and accessed during the answering process.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. **DocumentProcessor**: Handles the initial processing of input documents, creating text chunks and embeddings.\n",
    "\n",
    "2. **KnowledgeGraph**: Constructs a graph representation of the processed documents, where nodes represent text chunks and edges represent relationships between them.\n",
    "\n",
    "3. **QueryEngine**: Manages the process of answering user queries by leveraging the knowledge graph and vector store.\n",
    "\n",
    "4. **Visualizer**: Creates a visual representation of the graph and the traversal path taken to answer a query.\n",
    "\n",
    "## Method Details\n",
    "\n",
    "1. **Document Processing**:\n",
    "   - Input documents are split into manageable chunks.\n",
    "   - Each chunk is embedded using a language model.\n",
    "   - A vector store is created from these embeddings for efficient similarity search.\n",
    "\n",
    "2. **Knowledge Graph Construction**:\n",
    "   - Graph nodes are created for each text chunk.\n",
    "   - Concepts are extracted from each chunk using a combination of NLP techniques and language models.\n",
    "   - Extracted concepts are lemmatized to improve matching.\n",
    "   - Edges are added between nodes based on semantic similarity and shared concepts.\n",
    "   - Edge weights are calculated to represent the strength of relationships.\n",
    "\n",
    "3. **Query Processing**:\n",
    "   - The user query is embedded and used to retrieve relevant documents from the vector store.\n",
    "   - A priority queue is initialized with the nodes corresponding to the most relevant documents.\n",
    "   - The system employs a Dijkstra-like algorithm to traverse the knowledge graph:\n",
    "     * Nodes are explored in order of their priority (strength of connection to the query).\n",
    "     * For each explored node:\n",
    "       - Its content is added to the context.\n",
    "       - The system checks if the current context provides a complete answer.\n",
    "       - If the answer is incomplete:\n",
    "         * The node's concepts are processed and added to a set of visited concepts.\n",
    "         * Neighboring nodes are explored, with their priorities updated based on edge weights.\n",
    "         * Nodes are added to the priority queue if a stronger connection is found.\n",
    "   - This process continues until a complete answer is found or the priority queue is exhausted.\n",
    "   - If no complete answer is found after traversing the graph, the system generates a final answer using the accumulated context and a large language model.\n",
    "\n",
    "4. **Visualization**:\n",
    "   - The knowledge graph is visualized with nodes representing text chunks and edges representing relationships.\n",
    "   - Edge colors indicate the strength of relationships (weights).\n",
    "   - The traversal path taken to answer a query is highlighted with curved, dashed arrows.\n",
    "   - Start and end nodes of the traversal are distinctly colored for easy identification.\n",
    "\n",
    "## Benefits of This Approach\n",
    "\n",
    "1. **Improved Context Awareness**: By representing knowledge as a graph, the system can maintain better context and make connections across different parts of the input documents.\n",
    "\n",
    "2. **Enhanced Retrieval**: The graph structure allows for more intelligent retrieval of information, going beyond simple keyword matching.\n",
    "\n",
    "3. **Explainable Results**: The visualization of the graph and traversal path provides insight into how the system arrived at its answer, improving transparency and trust.\n",
    "\n",
    "4. **Flexible Knowledge Representation**: The graph structure can easily incorporate new information and relationships as they become available.\n",
    "\n",
    "5. **Efficient Information Traversal**: The weighted edges in the graph allow the system to prioritize the most relevant information pathways when answering queries.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "GraphRAG represents a significant advancement in retrieval-augmented generation systems. By incorporating a graph-based knowledge representation and intelligent traversal mechanisms, it offers improved context awareness, more accurate retrieval, and enhanced explainability. The system's ability to visualize its decision-making process provides valuable insights into its operation, making it a powerful tool for both end-users and developers. As natural language processing and graph-based AI continue to evolve, systems like GraphRAG pave the way for more sophisticated and capable question-answering technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/graph_rag.svg\" alt=\"graph RAG\" style=\"width:100%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deepeval in d:\\rag_techniques\\venv\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: requests in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (2.32.3)\n",
      "Requirement already satisfied: tqdm in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (4.66.5)\n",
      "Requirement already satisfied: pytest in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (8.3.2)\n",
      "Requirement already satisfied: tabulate in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (0.9.0)\n",
      "Requirement already satisfied: typer in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (0.12.5)\n",
      "Requirement already satisfied: rich in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (13.8.0)\n",
      "Requirement already satisfied: protobuf in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (5.29.3)\n",
      "Requirement already satisfied: pydantic in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (2.9.0)\n",
      "Requirement already satisfied: sentry-sdk in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (2.13.0)\n",
      "Requirement already satisfied: pytest-repeat in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (0.9.3)\n",
      "Requirement already satisfied: pytest-xdist in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (3.6.1)\n",
      "Requirement already satisfied: portalocker in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (2.10.1)\n",
      "Requirement already satisfied: langchain in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (0.2.16)\n",
      "Requirement already satisfied: llama-index in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (0.11.7)\n",
      "Requirement already satisfied: langchain-core in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (0.2.43)\n",
      "Requirement already satisfied: langchain_openai in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (0.1.23)\n",
      "Requirement already satisfied: langchain-community in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (0.2.16)\n",
      "Requirement already satisfied: docx2txt~=0.8 in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (0.8)\n",
      "Requirement already satisfied: importlib-metadata>=6.0.2 in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (7.0.0)\n",
      "Requirement already satisfied: tenacity<=9.0.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (8.4.2)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.24.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.24.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (1.24.0)\n",
      "Collecting grpcio==1.67.1 (from deepeval)\n",
      "  Using cached grpcio-1.67.1-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: nest-asyncio in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (1.6.0)\n",
      "Requirement already satisfied: datasets in d:\\rag_techniques\\venv\\lib\\site-packages (from deepeval) (2.21.0)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\rag_techniques\\venv\\lib\\site-packages (from importlib-metadata>=6.0.2->deepeval) (3.20.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in d:\\rag_techniques\\venv\\lib\\site-packages (from opentelemetry-api<2.0.0,>=1.24.0->deepeval) (1.2.14)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in d:\\rag_techniques\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.65.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.24.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.24.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.24.0)\n",
      "Collecting protobuf (from deepeval)\n",
      "  Using cached protobuf-4.25.6-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.45b0 in d:\\rag_techniques\\venv\\lib\\site-packages (from opentelemetry-sdk<2.0.0,>=1.24.0->deepeval) (0.45b0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in d:\\rag_techniques\\venv\\lib\\site-packages (from opentelemetry-sdk<2.0.0,>=1.24.0->deepeval) (4.12.2)\n",
      "Requirement already satisfied: filelock in d:\\rag_techniques\\venv\\lib\\site-packages (from datasets->deepeval) (3.16.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\rag_techniques\\venv\\lib\\site-packages (from datasets->deepeval) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from datasets->deepeval) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from datasets->deepeval) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\rag_techniques\\venv\\lib\\site-packages (from datasets->deepeval) (2.2.2)\n",
      "Requirement already satisfied: xxhash in d:\\rag_techniques\\venv\\lib\\site-packages (from datasets->deepeval) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in d:\\rag_techniques\\venv\\lib\\site-packages (from datasets->deepeval) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->deepeval) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in d:\\rag_techniques\\venv\\lib\\site-packages (from datasets->deepeval) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in d:\\rag_techniques\\venv\\lib\\site-packages (from datasets->deepeval) (0.24.6)\n",
      "Requirement already satisfied: packaging in d:\\rag_techniques\\venv\\lib\\site-packages (from datasets->deepeval) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\rag_techniques\\venv\\lib\\site-packages (from datasets->deepeval) (6.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\rag_techniques\\venv\\lib\\site-packages (from requests->deepeval) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\rag_techniques\\venv\\lib\\site-packages (from requests->deepeval) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\rag_techniques\\venv\\lib\\site-packages (from requests->deepeval) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\rag_techniques\\venv\\lib\\site-packages (from requests->deepeval) (2024.8.30)\n",
      "Requirement already satisfied: colorama in d:\\rag_techniques\\venv\\lib\\site-packages (from tqdm->deepeval) (0.4.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\rag_techniques\\venv\\lib\\site-packages (from langchain->deepeval) (2.0.34)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from langchain->deepeval) (0.2.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in d:\\rag_techniques\\venv\\lib\\site-packages (from langchain->deepeval) (0.1.147)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\rag_techniques\\venv\\lib\\site-packages (from langchain-core->deepeval) (1.33)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from pydantic->deepeval) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.2 in d:\\rag_techniques\\venv\\lib\\site-packages (from pydantic->deepeval) (2.23.2)\n",
      "Requirement already satisfied: tzdata in d:\\rag_techniques\\venv\\lib\\site-packages (from pydantic->deepeval) (2024.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\rag_techniques\\venv\\lib\\site-packages (from langchain-community->deepeval) (0.6.7)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.40.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from langchain_openai->deepeval) (1.44.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in d:\\rag_techniques\\venv\\lib\\site-packages (from langchain_openai->deepeval) (0.7.0)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.4.0,>=0.3.1 in d:\\rag_techniques\\venv\\lib\\site-packages (from llama-index->deepeval) (0.3.1)\n",
      "Requirement already satisfied: llama-index-cli<0.4.0,>=0.3.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from llama-index->deepeval) (0.3.0)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.7 in d:\\rag_techniques\\venv\\lib\\site-packages (from llama-index->deepeval) (0.11.7)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.3.0,>=0.2.4 in d:\\rag_techniques\\venv\\lib\\site-packages (from llama-index->deepeval) (0.2.4)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.3.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from llama-index->deepeval) (0.3.0)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in d:\\rag_techniques\\venv\\lib\\site-packages (from llama-index->deepeval) (0.9.48.post3)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.3.0,>=0.2.3 in d:\\rag_techniques\\venv\\lib\\site-packages (from llama-index->deepeval) (0.2.3)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from llama-index->deepeval) (0.2.0)\n",
      "Requirement already satisfied: llama-index-program-openai<0.3.0,>=0.2.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from llama-index->deepeval) (0.2.0)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.3.0,>=0.2.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from llama-index->deepeval) (0.2.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.3.0,>=0.2.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from llama-index->deepeval) (0.2.1)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.3.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from llama-index->deepeval) (0.3.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in d:\\rag_techniques\\venv\\lib\\site-packages (from llama-index->deepeval) (3.9.1)\n",
      "Requirement already satisfied: pywin32>=226 in d:\\rag_techniques\\venv\\lib\\site-packages (from portalocker->deepeval) (308)\n",
      "Requirement already satisfied: iniconfig in d:\\rag_techniques\\venv\\lib\\site-packages (from pytest->deepeval) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in d:\\rag_techniques\\venv\\lib\\site-packages (from pytest->deepeval) (1.5.0)\n",
      "Requirement already satisfied: execnet>=2.1 in d:\\rag_techniques\\venv\\lib\\site-packages (from pytest-xdist->deepeval) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from rich->deepeval) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from rich->deepeval) (2.18.0)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from typer->deepeval) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from typer->deepeval) (1.5.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from aiohttp->datasets->deepeval) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\rag_techniques\\venv\\lib\\site-packages (from aiohttp->datasets->deepeval) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from aiohttp->datasets->deepeval) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\rag_techniques\\venv\\lib\\site-packages (from aiohttp->datasets->deepeval) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\rag_techniques\\venv\\lib\\site-packages (from aiohttp->datasets->deepeval) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from aiohttp->datasets->deepeval) (1.10.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->deepeval) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->deepeval) (0.9.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in d:\\rag_techniques\\venv\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api<2.0.0,>=1.24.0->deepeval) (1.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\rag_techniques\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core->deepeval) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain->deepeval) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\rag_techniques\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain->deepeval) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain->deepeval) (1.0.0)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in d:\\rag_techniques\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index->deepeval) (1.0.8)\n",
      "Requirement already satisfied: networkx>=3.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index->deepeval) (3.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index->deepeval) (10.4.0)\n",
      "Requirement already satisfied: llama-cloud>=0.0.11 in d:\\rag_techniques\\venv\\lib\\site-packages (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index->deepeval) (0.0.17)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in d:\\rag_techniques\\venv\\lib\\site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index->deepeval) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in d:\\rag_techniques\\venv\\lib\\site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index->deepeval) (4.3.1)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in d:\\rag_techniques\\venv\\lib\\site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index->deepeval) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from llama-index-readers-llama-parse>=0.3.0->llama-index->deepeval) (0.5.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\rag_techniques\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->deepeval) (0.1.2)\n",
      "Requirement already satisfied: joblib in d:\\rag_techniques\\venv\\lib\\site-packages (from nltk>3.8.1->llama-index->deepeval) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\rag_techniques\\venv\\lib\\site-packages (from nltk>3.8.1->llama-index->deepeval) (2024.7.24)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from openai<2.0.0,>=1.40.0->langchain_openai->deepeval) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from openai<2.0.0,>=1.40.0->langchain_openai->deepeval) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from openai<2.0.0,>=1.40.0->langchain_openai->deepeval) (0.5.0)\n",
      "Requirement already satisfied: sniffio in d:\\rag_techniques\\venv\\lib\\site-packages (from openai<2.0.0,>=1.40.0->langchain_openai->deepeval) (1.3.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\rag_techniques\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->deepeval) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\rag_techniques\\venv\\lib\\site-packages (from pandas->datasets->deepeval) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\rag_techniques\\venv\\lib\\site-packages (from pandas->datasets->deepeval) (2024.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\rag_techniques\\venv\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index->deepeval) (2.6)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\rag_techniques\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->deepeval) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\rag_techniques\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->deepeval) (0.14.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\rag_techniques\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->deepeval) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->deepeval) (1.0.0)\n",
      "Using cached grpcio-1.67.1-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "Using cached protobuf-4.25.6-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Installing collected packages: protobuf, grpcio\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.3\n",
      "    Uninstalling protobuf-5.29.3:\n",
      "      Successfully uninstalled protobuf-5.29.3\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.70.0\n",
      "    Uninstalling grpcio-1.70.0:\n",
      "      Successfully uninstalled grpcio-1.70.0\n",
      "Successfully installed grpcio-1.70.0 protobuf-5.29.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-tools 1.70.0 requires grpcio>=1.70.0, but you have grpcio 1.67.1 which is incompatible.\n",
      "grpcio-tools 1.70.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.6 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qdrant-client in d:\\rag_techniques\\venv\\lib\\site-packages (1.13.2)\n",
      "Requirement already satisfied: langchain-qdrant in d:\\rag_techniques\\venv\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from qdrant-client) (1.67.1)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from qdrant-client) (1.70.0)\n",
      "Requirement already satisfied: httpx>=0.20.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.27.2)\n",
      "Requirement already satisfied: numpy>=1.26 in d:\\rag_techniques\\venv\\lib\\site-packages (from qdrant-client) (1.26.4)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from qdrant-client) (2.10.1)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in d:\\rag_techniques\\venv\\lib\\site-packages (from qdrant-client) (2.9.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in d:\\rag_techniques\\venv\\lib\\site-packages (from qdrant-client) (2.2.2)\n",
      "Requirement already satisfied: langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 in d:\\rag_techniques\\venv\\lib\\site-packages (from langchain-qdrant) (0.2.43)\n",
      "Collecting protobuf<6.0dev,>=5.26.1 (from grpcio-tools>=1.41.0->qdrant-client)\n",
      "  Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting grpcio>=1.41.0 (from qdrant-client)\n",
      "  Using cached grpcio-1.70.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: setuptools in d:\\rag_techniques\\venv\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client) (75.8.0)\n",
      "Requirement already satisfied: anyio in d:\\rag_techniques\\venv\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.4.0)\n",
      "Requirement already satisfied: certifi in d:\\rag_techniques\\venv\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\rag_techniques\\venv\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.5)\n",
      "Requirement already satisfied: idna in d:\\rag_techniques\\venv\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.8)\n",
      "Requirement already satisfied: sniffio in d:\\rag_techniques\\venv\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\rag_techniques\\venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in d:\\rag_techniques\\venv\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\rag_techniques\\venv\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\rag_techniques\\venv\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in d:\\rag_techniques\\venv\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (0.1.147)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\rag_techniques\\venv\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (24.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (8.4.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\rag_techniques\\venv\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (4.12.2)\n",
      "Requirement already satisfied: pywin32>=226 in d:\\rag_techniques\\venv\\lib\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant-client) (308)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from pydantic>=1.10.8->qdrant-client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.2 in d:\\rag_techniques\\venv\\lib\\site-packages (from pydantic>=1.10.8->qdrant-client) (2.23.2)\n",
      "Requirement already satisfied: tzdata in d:\\rag_techniques\\venv\\lib\\site-packages (from pydantic>=1.10.8->qdrant-client) (2024.1)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in d:\\rag_techniques\\venv\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in d:\\rag_techniques\\venv\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\rag_techniques\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\rag_techniques\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (3.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\rag_techniques\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\rag_techniques\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\rag_techniques\\venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.112->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (3.3.2)\n",
      "Using cached grpcio-1.70.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Installing collected packages: protobuf, grpcio\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.6\n",
      "    Uninstalling protobuf-4.25.6:\n",
      "      Successfully uninstalled protobuf-4.25.6\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.67.1\n",
      "    Uninstalling grpcio-1.67.1:\n",
      "      Successfully uninstalled grpcio-1.67.1\n",
      "Successfully installed grpcio-1.67.1 protobuf-4.25.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "deepeval 2.4.0 requires grpcio==1.67.1, but you have grpcio 1.70.0 which is incompatible.\n",
      "opentelemetry-proto 1.24.0 requires protobuf<5.0,>=3.19, but you have protobuf 5.29.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade deepeval \n",
    "!pip install qdrant-client langchain-qdrant jq markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-20T11:47:12.549256Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAG_Techniques\\venv\\Lib\\site-packages\\deepeval\\__init__.py:53: UserWarning: You are using deepeval version 2.4.0, however version 2.4.1 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List, Tuple, Dict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import spacy\n",
    "import heapq\n",
    "\n",
    "from spacy.cli import download\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.it import Italian\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path since we work with notebooks\n",
    "from helper_functions import *\n",
    "from evaluation.evalute_rag import *\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "os.environ[\"QDRAND_PORT\"]=\"6333\"\n",
    "os.environ[\"QDRANT_URL\"]=\"http://localhost\"\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QDrant Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client.models import Distance\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams\n",
    "import uuid\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "def qdrant_connection():\n",
    "    \"\"\"\n",
    "    Establishes a connection to the Qdrant client using environment variables.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the QDRANT_URL or QDRAND_PORT environment variables are not set.\n",
    "\n",
    "    Returns:\n",
    "        QdrantClient: An instance of the Qdrant client.\n",
    "    \"\"\"\n",
    "    # Check if the QDRANT_URL environment variable is set\n",
    "    url = os.getenv(\"QDRANT_URL\")\n",
    "    if url is None or url == \"\":\n",
    "        raise Exception(\"The QDRANT_URL environment variable must be provided\")\n",
    "\n",
    "    # Check if the QDRAND_PORT environment variable is set\n",
    "    port = os.getenv(\"QDRAND_PORT\")\n",
    "    if port is None or port == \"\":\n",
    "        raise Exception(\"The QDRAND_PORT environment variable must be provided\")\n",
    "\n",
    "    # Qdrant client\n",
    "    return QdrantClient(url=url, port=int(port))\n",
    "\n",
    "# embedchain = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "qd_client = qdrant_connection()\n",
    "\n",
    "def vector_store(collection: str = \"graph_rag\", embedchain: OpenAIEmbeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")) -> QdrantVectorStore:\n",
    "    \"\"\"\n",
    "    Get or create a Qdrant vector store for the specified collection.\n",
    "\n",
    "    Args:\n",
    "        collection (str, optional): The name of the collection. Defaults to COLLECTION_NAME.\n",
    "\n",
    "    Returns:\n",
    "        QdrantVectorStore: The Qdrant vector store for the specified collection.\n",
    "    \"\"\"\n",
    "    collections_list = qd_client.get_collections()\n",
    "    existing_collections = [col.name for col in collections_list.collections]\n",
    "\n",
    "    collection = f\"{collection.lower()}_test_jupyter\"\n",
    "\n",
    "    # Check if the collection exists, if not, create it\n",
    "    if collection not in existing_collections:\n",
    "        qd_client.create_collection(\n",
    "            collection_name=collection,\n",
    "            vectors_config=VectorParams(size=1536, distance=Distance.COSINE)\n",
    "        )\n",
    "\n",
    "    # Initialize Qdrant vector store from the existing collection\n",
    "    return QdrantVectorStore.from_existing_collection(\n",
    "        embedding=embedchain,\n",
    "        collection_name=collection,\n",
    "        url=os.getenv(\"QDRANT_URL\")\n",
    "    )\n",
    "\n",
    "def create_vs(documents: List[Document], embedchain: OpenAIEmbeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")):\n",
    "    vs = vector_store()\n",
    "    new_documents = []\n",
    "    for doc in documents:\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        d = Document(\n",
    "            page_content=doc.page_content,\n",
    "            id=doc_id\n",
    "        )\n",
    "        new_documents.append(d)\n",
    "    vs.add_documents(new_documents)\n",
    "    return vs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the document processor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T11:34:39.817233Z",
     "start_time": "2025-02-20T11:34:39.804939Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the DocumentProcessor class\n",
    "class DocumentProcessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the DocumentProcessor with a text splitter and OpenAI embeddings.\n",
    "        \n",
    "        Attributes:\n",
    "        - text_splitter: An instance of RecursiveCharacterTextSplitter with specified chunk size and overlap.\n",
    "        - embeddings: An instance of OpenAIEmbeddings used for embedding documents.\n",
    "        \"\"\"\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    def process_documents(self, documents):\n",
    "        \"\"\"\n",
    "        Processes a list of documents by splitting them into smaller chunks and creating a vector store.\n",
    "        \n",
    "        Args:\n",
    "        - documents (list of str): A list of documents to be processed.\n",
    "        \n",
    "        Returns:\n",
    "        - tuple: A tuple containing:\n",
    "          - splits (list of str): The list of split document chunks.\n",
    "          - vector_store (FAISS): A FAISS vector store created from the split document chunks and their embeddings.\n",
    "        \"\"\"\n",
    "        splits = self.text_splitter.split_documents(documents)\n",
    "        vector_store = create_vs(splits, self.embeddings)\n",
    "        # vector_store = FAISS.from_documents(splits, self.embeddings)\n",
    "        return splits, vector_store\n",
    "\n",
    "    def create_embeddings_batch(self, texts, batch_size=32):\n",
    "        \"\"\"\n",
    "        Creates embeddings for a list of texts in batches.\n",
    "        \n",
    "        Args:\n",
    "        - texts (list of str): A list of texts to be embedded.\n",
    "        - batch_size (int, optional): The number of texts to process in each batch. Default is 32.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: An array of embeddings for the input texts.\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.embeddings.embed_documents(batch)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def compute_similarity_matrix(self, embeddings):\n",
    "        \"\"\"\n",
    "        Computes a cosine similarity matrix for a given set of embeddings.\n",
    "        \n",
    "        Args:\n",
    "        - embeddings (numpy.ndarray): An array of embeddings.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: A cosine similarity matrix for the input embeddings.\n",
    "        \"\"\"\n",
    "        return cosine_similarity(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the knowledge graph class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T11:34:39.865123Z",
     "start_time": "2025-02-20T11:34:39.840041Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from spacy.cli import download\n",
    "\n",
    "# Define the Concepts class\n",
    "class Concepts(BaseModel):\n",
    "    concepts_list: List[str] = Field(description=\"List of concepts\")\n",
    "\n",
    "# Define the KnowledgeGraph class\n",
    "class KnowledgeGraph:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the KnowledgeGraph with a graph, lemmatizer, and NLP model.\n",
    "        \n",
    "        Attributes:\n",
    "        - graph: An instance of a networkx Graph.\n",
    "        - lemmatizer: An instance of WordNetLemmatizer.\n",
    "        - concept_cache: A dictionary to cache extracted concepts.\n",
    "        - nlp: An instance of a spaCy NLP model.\n",
    "        - edges_threshold: A float value that sets the threshold for adding edges based on similarity.\n",
    "        \"\"\"\n",
    "        self.graph = nx.Graph()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.concept_cache = {}\n",
    "        self.nlp = self._load_spacy_model()\n",
    "        self.edges_threshold = 0.8\n",
    "\n",
    "    def build_graph(self, splits, llm, embedding_model):\n",
    "        \"\"\"\n",
    "        Builds the knowledge graph by adding nodes, creating embeddings, extracting concepts, and adding edges.\n",
    "        \n",
    "        Args:\n",
    "        - splits (list): A list of document splits.\n",
    "        - llm: An instance of a large language model.\n",
    "        - embedding_model: An instance of an embedding model.\n",
    "        \n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        self._add_nodes(splits)\n",
    "        embeddings = self._create_embeddings(splits, embedding_model)\n",
    "        self._extract_concepts(splits, llm)\n",
    "        self._add_edges(embeddings)\n",
    "\n",
    "    def _add_nodes(self, splits):\n",
    "        \"\"\"\n",
    "        Adds nodes to the graph from the document splits.\n",
    "        \n",
    "        Args:\n",
    "        - splits (list): A list of document splits.\n",
    "        \n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        for i, split in enumerate(splits):\n",
    "            self.graph.add_node(i, content=split.page_content)\n",
    "\n",
    "    def _create_embeddings(self, splits, embedding_model):\n",
    "        \"\"\"\n",
    "        Creates embeddings for the document splits using the embedding model.\n",
    "        \n",
    "        Args:\n",
    "        - splits (list): A list of document splits.\n",
    "        - embedding_model: An instance of an embedding model.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: An array of embeddings for the document splits.\n",
    "        \"\"\"\n",
    "        texts = [split.page_content for split in splits]\n",
    "        return embedding_model.embed_documents(texts)\n",
    "\n",
    "    def _compute_similarities(self, embeddings):\n",
    "        \"\"\"\n",
    "        Computes the cosine similarity matrix for the embeddings.\n",
    "        \n",
    "        Args:\n",
    "        - embeddings (numpy.ndarray): An array of embeddings.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: A cosine similarity matrix for the embeddings.\n",
    "        \"\"\"\n",
    "        return cosine_similarity(embeddings)\n",
    "\n",
    "    def _load_spacy_model(self):\n",
    "        \"\"\"\n",
    "        Loads the spaCy NLP model, downloading it if necessary.\n",
    "        \n",
    "        Args:\n",
    "        - None\n",
    "        \n",
    "        Returns:\n",
    "        - spacy.Language: An instance of a spaCy NLP model.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return spacy.load(\"en_core_web_sm\")\n",
    "        except OSError:\n",
    "            print(\"Downloading spaCy model...\")\n",
    "            download(\"en_core_web_sm\")\n",
    "            return spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def _extract_concepts_and_entities(self, content, llm):\n",
    "        \"\"\"\n",
    "        Extracts concepts and named entities from the content using spaCy and a large language model.\n",
    "        \n",
    "        Args:\n",
    "        - content (str): The content from which to extract concepts and entities.\n",
    "        - llm: An instance of a large language model.\n",
    "        \n",
    "        Returns:\n",
    "        - list: A list of extracted concepts and entities.\n",
    "        \"\"\"\n",
    "        if content in self.concept_cache:\n",
    "            return self.concept_cache[content]\n",
    "        \n",
    "        # Extract named entities using spaCy\n",
    "        doc = self.nlp(content)\n",
    "        named_entities = [ent.text for ent in doc.ents if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\", \"WORK_OF_ART\"]]\n",
    "        \n",
    "        # Extract general concepts using LLM\n",
    "        concept_extraction_prompt = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=\"Extract key concepts (excluding named entities) from the following text:\\n\\n{text}\\n\\nKey concepts:\"\n",
    "        )\n",
    "        concept_chain = concept_extraction_prompt | llm.with_structured_output(Concepts)\n",
    "        general_concepts = concept_chain.invoke({\"text\": content}).concepts_list\n",
    "        \n",
    "        # Combine named entities and general concepts\n",
    "        all_concepts = list(set(named_entities + general_concepts))\n",
    "        \n",
    "        self.concept_cache[content] = all_concepts\n",
    "        return all_concepts\n",
    "\n",
    "    def _extract_concepts(self, splits, llm):\n",
    "        \"\"\"\n",
    "        Extracts concepts for all document splits using multi-threading.\n",
    "        \n",
    "        Args:\n",
    "        - splits (list): A list of document splits.\n",
    "        - llm: An instance of a large language model.\n",
    "        \n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            future_to_node = {executor.submit(self._extract_concepts_and_entities, split.page_content, llm): i \n",
    "                              for i, split in enumerate(splits)}\n",
    "            \n",
    "            for future in tqdm(as_completed(future_to_node), total=len(splits), desc=\"Extracting concepts and entities\"):\n",
    "                node = future_to_node[future]\n",
    "                concepts = future.result()\n",
    "                self.graph.nodes[node]['concepts'] = concepts\n",
    "\n",
    "    def _add_edges(self, embeddings):\n",
    "        \"\"\"\n",
    "        Adds edges to the graph based on the similarity of embeddings and shared concepts.\n",
    "        \n",
    "        Args:\n",
    "        - embeddings (numpy.ndarray): An array of embeddings for the document splits.\n",
    "        \n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        similarity_matrix = self._compute_similarities(embeddings)\n",
    "        num_nodes = len(self.graph.nodes)\n",
    "        \n",
    "        for node1 in tqdm(range(num_nodes), desc=\"Adding edges\"):\n",
    "            for node2 in range(node1 + 1, num_nodes):\n",
    "                similarity_score = similarity_matrix[node1][node2]\n",
    "                if similarity_score > self.edges_threshold:\n",
    "                    shared_concepts = set(self.graph.nodes[node1]['concepts']) & set(self.graph.nodes[node2]['concepts'])\n",
    "                    edge_weight = self._calculate_edge_weight(node1, node2, similarity_score, shared_concepts)\n",
    "                    self.graph.add_edge(node1, node2, weight=edge_weight, \n",
    "                                        similarity=similarity_score,\n",
    "                                        shared_concepts=list(shared_concepts))\n",
    "\n",
    "    def _calculate_edge_weight(self, node1, node2, similarity_score, shared_concepts, alpha=0.7, beta=0.3):\n",
    "        \"\"\"\n",
    "        Calculates the weight of an edge based on similarity score and shared concepts.\n",
    "        \n",
    "        Args:\n",
    "        - node1 (int): The first node.\n",
    "        - node2 (int): The second node.\n",
    "        - similarity_score (float): The similarity score between the nodes.\n",
    "        - shared_concepts (set): The set of shared concepts between the nodes.\n",
    "        - alpha (float, optional): The weight of the similarity score. Default is 0.7.\n",
    "        - beta (float, optional): The weight of the shared concepts. Default is 0.3.\n",
    "        \n",
    "        Returns:\n",
    "        - float: The calculated weight of the edge.\n",
    "        \"\"\"\n",
    "        max_possible_shared = min(len(self.graph.nodes[node1]['concepts']), len(self.graph.nodes[node2]['concepts']))\n",
    "        normalized_shared_concepts = len(shared_concepts) / max_possible_shared if max_possible_shared > 0 else 0\n",
    "        return alpha * similarity_score + beta * normalized_shared_concepts\n",
    "\n",
    "    def _lemmatize_concept(self, concept):\n",
    "        \"\"\"\n",
    "        Lemmatizes a given concept.\n",
    "        \n",
    "        Args:\n",
    "        - concept (str): The concept to be lemmatized.\n",
    "        \n",
    "        Returns:\n",
    "        - str: The lemmatized concept.\n",
    "        \"\"\"\n",
    "        return ' '.join([self.lemmatizer.lemmatize(word) for word in concept.lower().split()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Query Engine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T11:34:40.948860Z",
     "start_time": "2025-02-20T11:34:40.001691Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from langchain.retrievers.document_compressors.chain_extract import LLMChainExtractor\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "\n",
    "# Define the AnswerCheck class\n",
    "class AnswerCheck(BaseModel):\n",
    "    is_complete: bool = Field(description=\"Whether the current context provides a complete answer to the query\")\n",
    "    answer: str = Field(description=\"The current answer based on the context, if any\")\n",
    "\n",
    "# Define the QueryEngine class\n",
    "class QueryEngine:\n",
    "    def __init__(self, vector_store, knowledge_graph, llm):\n",
    "        self.vector_store = vector_store\n",
    "        self.knowledge_graph = knowledge_graph\n",
    "        self.llm = llm\n",
    "        self.max_context_length = 4000\n",
    "        self.answer_check_chain = self._create_answer_check_chain()\n",
    "\n",
    "    def _create_answer_check_chain(self):\n",
    "        \"\"\"\n",
    "        Creates a chain to check if the context provides a complete answer to the query.\n",
    "        \n",
    "        Args:\n",
    "        - None\n",
    "        \n",
    "        Returns:\n",
    "        - Chain: A chain to check if the context provides a complete answer.\n",
    "        \"\"\"\n",
    "        answer_check_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"context\"],\n",
    "            template=\"Given the query: '{query}'\\n\\nAnd the current context:\\n{context}\\n\\nDoes this context provide a complete answer to the query? If yes, provide the answer. If no, state that the answer is incomplete.\\n\\nIs complete answer (Yes/No):\\nAnswer (if complete):\"\n",
    "        )\n",
    "        return answer_check_prompt | self.llm.with_structured_output(AnswerCheck)\n",
    "\n",
    "    def _check_answer(self, query: str, context: str) -> Tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Checks if the current context provides a complete answer to the query.\n",
    "        \n",
    "        Args:\n",
    "        - query (str): The query to be answered.\n",
    "        - context (str): The current context.\n",
    "        \n",
    "        Returns:\n",
    "        - tuple: A tuple containing:\n",
    "          - is_complete (bool): Whether the context provides a complete answer.\n",
    "          - answer (str): The answer based on the context, if complete.\n",
    "        \"\"\"\n",
    "        response = self.answer_check_chain.invoke({\"query\": query, \"context\": context})\n",
    "        return response.is_complete, response.answer\n",
    "\n",
    "  \n",
    "\n",
    "    def _expand_context(self, query: str, relevant_docs) -> Tuple[str, List[int], Dict[int, str], str]:\n",
    "        \"\"\"\n",
    "        Expands the context by traversing the knowledge graph using a Dijkstra-like approach.\n",
    "        \n",
    "        This method implements a modified version of Dijkstra's algorithm to explore the knowledge graph,\n",
    "        prioritizing the most relevant and strongly connected information. The algorithm works as follows:\n",
    "\n",
    "        1. Initialize:\n",
    "           - Start with nodes corresponding to the most relevant documents.\n",
    "           - Use a priority queue to manage the traversal order, where priority is based on connection strength.\n",
    "           - Maintain a dictionary of best known \"distances\" (inverse of connection strengths) to each node.\n",
    "\n",
    "        2. Traverse:\n",
    "           - Always explore the node with the highest priority (strongest connection) next.\n",
    "           - For each node, check if we've found a complete answer.\n",
    "           - Explore the node's neighbors, updating their priorities if a stronger connection is found.\n",
    "\n",
    "        3. Concept Handling:\n",
    "           - Track visited concepts to guide the exploration towards new, relevant information.\n",
    "           - Expand to neighbors only if they introduce new concepts.\n",
    "\n",
    "        4. Termination:\n",
    "           - Stop if a complete answer is found.\n",
    "           - Continue until the priority queue is empty (all reachable nodes explored).\n",
    "\n",
    "        This approach ensures that:\n",
    "        - We prioritize the most relevant and strongly connected information.\n",
    "        - We explore new concepts systematically.\n",
    "        - We find the most relevant answer by following the strongest connections in the knowledge graph.\n",
    "\n",
    "        Args:\n",
    "        - query (str): The query to be answered.\n",
    "        - relevant_docs (List[Document]): A list of relevant documents to start the traversal.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: A tuple containing:\n",
    "          - expanded_context (str): The accumulated context from traversed nodes.\n",
    "          - traversal_path (List[int]): The sequence of node indices visited.\n",
    "          - filtered_content (Dict[int, str]): A mapping of node indices to their content.\n",
    "          - final_answer (str): The final answer found, if any.\n",
    "        \"\"\"\n",
    "        # Initialize variables\n",
    "        expanded_context = \"\"\n",
    "        traversal_path = []\n",
    "        visited_concepts = set()\n",
    "        filtered_content = {}\n",
    "        final_answer = \"\"\n",
    "        \n",
    "        priority_queue = []\n",
    "        distances = {}  # Stores the best known \"distance\" (inverse of connection strength) to each node\n",
    "        \n",
    "        print(\"\\nTraversing the knowledge graph:\")\n",
    "        \n",
    "        # Initialize priority queue with closest nodes from relevant docs\n",
    "        for doc in relevant_docs:\n",
    "            # Find the most similar node in the knowledge graph for each relevant document\n",
    "            closest_nodes = self.vector_store.similarity_search_with_score(doc.page_content, k=1)\n",
    "            closest_node_content, similarity_score = closest_nodes[0]\n",
    "            \n",
    "            # Get the corresponding node in our knowledge graph\n",
    "            closest_node = next(n for n in self.knowledge_graph._graph.nodes if self.knowledge_graph._graph.nodes[n]['content'] == closest_node_content.page_content)\n",
    "            \n",
    "            # Initialize priority (inverse of similarity score for min-heap behavior)\n",
    "            priority = 1 / similarity_score\n",
    "            heapq.heappush(priority_queue, (priority, closest_node))\n",
    "            distances[closest_node] = priority\n",
    "        \n",
    "        step = 0\n",
    "        while priority_queue:\n",
    "            # Get the node with the highest priority (lowest distance value)\n",
    "            current_priority, current_node = heapq.heappop(priority_queue)\n",
    "            \n",
    "            # Skip if we've already found a better path to this node\n",
    "            if current_priority > distances.get(current_node, float('inf')):\n",
    "                continue\n",
    "            \n",
    "            if current_node not in traversal_path:\n",
    "                step += 1\n",
    "                traversal_path.append(current_node)\n",
    "                node_content = self.knowledge_graph._graph.nodes[current_node]['content']\n",
    "                node_concepts = self.knowledge_graph._graph.nodes[current_node]['concepts']\n",
    "                \n",
    "                # Add node content to our accumulated context\n",
    "                filtered_content[current_node] = node_content\n",
    "                expanded_context += \"\\n\" + node_content if expanded_context else node_content\n",
    "                \n",
    "                # Log the current step for debugging and visualization\n",
    "                print(f\"\\nStep {step} - Node {current_node}:\")\n",
    "                print(f\"Content: {node_content[:100]}...\") \n",
    "                print(f\"Concepts: {', '.join(node_concepts)}\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                # Check if we have a complete answer with the current context\n",
    "                is_complete, answer = self._check_answer(query, expanded_context)\n",
    "                if is_complete:\n",
    "                    final_answer = answer\n",
    "                    break\n",
    "                \n",
    "                # Process the concepts of the current node\n",
    "                node_concepts_set = set(self.knowledge_graph._lemmatize_concept(c) for c in node_concepts)\n",
    "                if not node_concepts_set.issubset(visited_concepts):\n",
    "                    visited_concepts.update(node_concepts_set)\n",
    "                    \n",
    "                    # Explore neighbors\n",
    "                    for neighbor in self.knowledge_graph._graph.neighbors(current_node):\n",
    "                        edge_data = self.knowledge_graph._graph[current_node][neighbor]\n",
    "                        edge_weight = edge_data['weight']\n",
    "                        \n",
    "                        # Calculate new distance (priority) to the neighbor\n",
    "                        # Note: We use 1 / edge_weight because higher weights mean stronger connections\n",
    "                        distance = current_priority + (1 / edge_weight)\n",
    "                        \n",
    "                        # If we've found a stronger connection to the neighbor, update its distance\n",
    "                        if distance < distances.get(neighbor, float('inf')):\n",
    "                            distances[neighbor] = distance\n",
    "                            heapq.heappush(priority_queue, (distance, neighbor))\n",
    "                            \n",
    "                            # Process the neighbor node if it's not already in our traversal path\n",
    "                            if neighbor not in traversal_path:\n",
    "                                step += 1\n",
    "                                traversal_path.append(neighbor)\n",
    "                                neighbor_content = self.knowledge_graph._graph.nodes[neighbor]['content']\n",
    "                                neighbor_concepts = self.knowledge_graph._graph.nodes[neighbor]['concepts']\n",
    "                                \n",
    "                                filtered_content[neighbor] = neighbor_content\n",
    "                                expanded_context += \"\\n\" + neighbor_content if expanded_context else neighbor_content\n",
    "                                \n",
    "                                # Log the neighbor node information\n",
    "                                print(f\"\\nStep {step} - Node {neighbor} (neighbor of {current_node}):\")\n",
    "                                print(f\"Content: {neighbor_content[:100]}...\")\n",
    "                                print(f\"Concepts: {', '.join(neighbor_concepts)}\")\n",
    "                                print(\"-\" * 50)\n",
    "                                \n",
    "                                # Check if we have a complete answer after adding the neighbor's content\n",
    "                                is_complete, answer = self._check_answer(query, expanded_context)\n",
    "                                if is_complete:\n",
    "                                    final_answer = answer\n",
    "                                    break\n",
    "                                \n",
    "                                # Process the neighbor's concepts\n",
    "                                neighbor_concepts_set = set(self.knowledge_graph._lemmatize_concept(c) for c in neighbor_concepts)\n",
    "                                if not neighbor_concepts_set.issubset(visited_concepts):\n",
    "                                    visited_concepts.update(neighbor_concepts_set)\n",
    "                \n",
    "                # If we found a final answer, break out of the main loop\n",
    "                if final_answer:\n",
    "                    break\n",
    "\n",
    "        # If we haven't found a complete answer, generate one using the LLM\n",
    "        if not final_answer:\n",
    "            print(\"\\nGenerating final answer...\")\n",
    "            response_prompt = PromptTemplate(\n",
    "                input_variables=[\"query\", \"context\"],\n",
    "                template=\"Based on the following context, please answer the query.\\n\\nContext: {context}\\n\\nQuery: {query}\\n\\nAnswer:\"\n",
    "            )\n",
    "            response_chain = response_prompt | self.llm\n",
    "            input_data = {\"query\": query, \"context\": expanded_context}\n",
    "            final_answer = response_chain.invoke(input_data)\n",
    "\n",
    "        return expanded_context, traversal_path, filtered_content, final_answer\n",
    "\n",
    "    def query(self, query: str) -> Tuple[str, List[int], Dict[int, str]]:\n",
    "        \"\"\"\n",
    "        Processes a query by retrieving relevant documents, expanding the context, and generating the final answer.\n",
    "        \n",
    "        Args:\n",
    "        - query (str): The query to be answered.\n",
    "        \n",
    "        Returns:\n",
    "        - tuple: A tuple containing:\n",
    "          - final_answer (str): The final answer to the query.\n",
    "          - traversal_path (list): The traversal path of nodes in the knowledge graph.\n",
    "          - filtered_content (dict): The filtered content of nodes.\n",
    "        \"\"\"\n",
    "        with get_openai_callback() as cb:\n",
    "            print(f\"\\nProcessing query: {query}\")\n",
    "            relevant_docs = self._retrieve_relevant_documents(query)\n",
    "            expanded_context, traversal_path, filtered_content, final_answer = self._expand_context(query, relevant_docs)\n",
    "            \n",
    "            if not final_answer:\n",
    "                print(\"\\nGenerating final answer...\")\n",
    "                response_prompt = PromptTemplate(\n",
    "                    input_variables=[\"query\", \"context\"],\n",
    "                    template=\"Based on the following context, please answer the query.\\n\\nContext: {context}\\n\\nQuery: {query}\\n\\nAnswer:\"\n",
    "                )\n",
    "                \n",
    "                response_chain = response_prompt | self.llm\n",
    "                input_data = {\"query\": query, \"context\": expanded_context}\n",
    "                response = response_chain.invoke(input_data)\n",
    "                final_answer = response\n",
    "            else:\n",
    "                print(\"\\nComplete answer found during traversal.\")\n",
    "            \n",
    "            print(f\"\\nFinal Answer: {final_answer}\")\n",
    "            print(f\"\\nTotal Tokens: {cb.total_tokens}\")\n",
    "            print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "            print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "            print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
    "        \n",
    "        return final_answer, traversal_path, filtered_content\n",
    "\n",
    "    def _retrieve_relevant_documents(self, query: str):\n",
    "        \"\"\"\n",
    "        Retrieves relevant documents based on the query using the vector store.\n",
    "        \n",
    "        Args:\n",
    "        - query (str): The query to be answered.\n",
    "        \n",
    "        Returns:\n",
    "        - list: A list of relevant documents.\n",
    "        \"\"\"\n",
    "        print(\"\\nRetrieving relevant documents...\")\n",
    "        retriever = self.vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "        compressor = LLMChainExtractor.from_llm(self.llm)\n",
    "        compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\n",
    "        return compression_retriever.invoke(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Visualizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T11:34:40.995133Z",
     "start_time": "2025-02-20T11:34:40.968945Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Define the Visualizer class\n",
    "class Visualizer:\n",
    "    @staticmethod\n",
    "    def visualize_traversal(graph, traversal_path):\n",
    "        \"\"\"\n",
    "        Visualizes the traversal path on the knowledge graph with nodes, edges, and traversal path highlighted.\n",
    "\n",
    "        Args:\n",
    "        - graph (networkx.Graph): The knowledge graph containing nodes and edges.\n",
    "        - traversal_path (list of int): The list of node indices representing the traversal path.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        traversal_graph = nx.DiGraph()\n",
    "        \n",
    "        # Add nodes and edges from the original graph\n",
    "        for node in graph.nodes():\n",
    "            traversal_graph.add_node(node)\n",
    "        for u, v, data in graph.edges(data=True):\n",
    "            traversal_graph.add_edge(u, v, **data)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(16, 12))\n",
    "        \n",
    "        # Generate positions for all nodes\n",
    "        pos = nx.spring_layout(traversal_graph, k=1, iterations=50)\n",
    "        \n",
    "        # Draw regular edges with color based on weight\n",
    "        edges = traversal_graph.edges()\n",
    "        edge_weights = [traversal_graph[u][v].get('weight', 0.5) for u, v in edges]\n",
    "        nx.draw_networkx_edges(traversal_graph, pos, \n",
    "                               edgelist=edges,\n",
    "                               edge_color=edge_weights,\n",
    "                               edge_cmap=plt.cm.Blues,\n",
    "                               width=2,\n",
    "                               ax=ax)\n",
    "        \n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(traversal_graph, pos, \n",
    "                               node_color='lightblue',\n",
    "                               node_size=3000,\n",
    "                               ax=ax)\n",
    "        \n",
    "        # Draw traversal path with curved arrows\n",
    "        edge_offset = 0.1\n",
    "        for i in range(len(traversal_path) - 1):\n",
    "            start = traversal_path[i]\n",
    "            end = traversal_path[i + 1]\n",
    "            start_pos = pos[start]\n",
    "            end_pos = pos[end]\n",
    "            \n",
    "            # Calculate control point for curve\n",
    "            mid_point = ((start_pos[0] + end_pos[0]) / 2, (start_pos[1] + end_pos[1]) / 2)\n",
    "            control_point = (mid_point[0] + edge_offset, mid_point[1] + edge_offset)\n",
    "            \n",
    "            # Draw curved arrow\n",
    "            arrow = patches.FancyArrowPatch(start_pos, end_pos,\n",
    "                                            connectionstyle=f\"arc3,rad={0.3}\",\n",
    "                                            color='red',\n",
    "                                            arrowstyle=\"->\",\n",
    "                                            mutation_scale=20,\n",
    "                                            linestyle='--',\n",
    "                                            linewidth=2,\n",
    "                                            zorder=4)\n",
    "            ax.add_patch(arrow)\n",
    "        \n",
    "        # Prepare labels for the nodes\n",
    "        labels = {}\n",
    "        for i, node in enumerate(traversal_path):\n",
    "            concepts = graph.nodes[node].get('concepts', [])\n",
    "            label = f\"{i + 1}. {concepts[0] if concepts else ''}\"\n",
    "            labels[node] = label\n",
    "        \n",
    "        for node in traversal_graph.nodes():\n",
    "            if node not in labels:\n",
    "                concepts = graph.nodes[node].get('concepts', [])\n",
    "                labels[node] = concepts[0] if concepts else ''\n",
    "        \n",
    "        # Draw labels\n",
    "        nx.draw_networkx_labels(traversal_graph, pos, labels, font_size=8, font_weight=\"bold\", ax=ax)\n",
    "        \n",
    "        # Highlight start and end nodes\n",
    "        start_node = traversal_path[0]\n",
    "        end_node = traversal_path[-1]\n",
    "        \n",
    "        nx.draw_networkx_nodes(traversal_graph, pos, \n",
    "                               nodelist=[start_node], \n",
    "                               node_color='lightgreen', \n",
    "                               node_size=3000,\n",
    "                               ax=ax)\n",
    "        \n",
    "        nx.draw_networkx_nodes(traversal_graph, pos, \n",
    "                               nodelist=[end_node], \n",
    "                               node_color='lightcoral', \n",
    "                               node_size=3000,\n",
    "                               ax=ax)\n",
    "        \n",
    "        ax.set_title(\"Graph Traversal Flow\")\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Add colorbar for edge weights\n",
    "        sm = plt.cm.ScalarMappable(cmap=plt.cm.Blues, norm=plt.Normalize(vmin=min(edge_weights), vmax=max(edge_weights)))\n",
    "        sm.set_array([])\n",
    "        cbar = fig.colorbar(sm, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "        cbar.set_label('Edge Weight', rotation=270, labelpad=15)\n",
    "        \n",
    "        # Add legend\n",
    "        regular_line = plt.Line2D([0], [0], color='blue', linewidth=2, label='Regular Edge')\n",
    "        traversal_line = plt.Line2D([0], [0], color='red', linewidth=2, linestyle='--', label='Traversal Path')\n",
    "        start_point = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightgreen', markersize=15, label='Start Node')\n",
    "        end_point = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightcoral', markersize=15, label='End Node')\n",
    "        legend = plt.legend(handles=[regular_line, traversal_line, start_point, end_point], loc='upper left', bbox_to_anchor=(0, 1), ncol=2)\n",
    "        legend.get_frame().set_alpha(0.8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def print_filtered_content(traversal_path, filtered_content):\n",
    "        \"\"\"\n",
    "        Prints the filtered content of visited nodes in the order of traversal.\n",
    "\n",
    "        Args:\n",
    "        - traversal_path (list of int): The list of node indices representing the traversal path.\n",
    "        - filtered_content (dict of int: str): A dictionary mapping node indices to their filtered content.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        print(\"\\nFiltered content of visited nodes in order of traversal:\")\n",
    "        for i, node in enumerate(traversal_path):\n",
    "            print(f\"\\nStep {i + 1} - Node {node}:\")\n",
    "            print(f\"Filtered Content: {filtered_content.get(node, 'No filtered content available')[:200]}...\")  # Print first 200 characters\n",
    "            print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the graph RAG class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T11:34:41.012698Z",
     "start_time": "2025-02-20T11:34:41.002615Z"
    }
   },
   "outputs": [],
   "source": [
    "class GraphRAG:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the GraphRAG system with components for document processing, knowledge graph construction,\n",
    "        querying, and visualization.\n",
    "        \n",
    "        Attributes:\n",
    "        - llm: An instance of a large language model (LLM) for generating responses.\n",
    "        - embedding_model: An instance of an embedding model for document embeddings.\n",
    "        - document_processor: An instance of the DocumentProcessor class for processing documents.\n",
    "        - knowledge_graph: An instance of the KnowledgeGraph class for building and managing the knowledge graph.\n",
    "        - query_engine: An instance of the QueryEngine class for handling queries (initialized as None).\n",
    "        - visualizer: An instance of the Visualizer class for visualizing the knowledge graph traversal.\n",
    "        \"\"\"\n",
    "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
    "        self.embedding_model = OpenAIEmbeddings()\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.knowledge_graph = KnowledgeGraph()\n",
    "        self.query_engine = None\n",
    "        self.visualizer = Visualizer()\n",
    "\n",
    "    def process_documents(self, documents):\n",
    "        \"\"\"\n",
    "        Processes a list of documents by splitting them into chunks, embedding them, and building a knowledge graph.\n",
    "        \n",
    "        Args:\n",
    "        - documents (list of str): A list of documents to be processed.\n",
    "        \n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        splits, vector_store = self.document_processor.process_documents(documents)\n",
    "        self.knowledge_graph.build_graph(splits, self.llm, self.embedding_model)\n",
    "        self.query_engine = QueryEngine(vector_store, self.knowledge_graph, self.llm)\n",
    "\n",
    "    def query(self, query: str):\n",
    "        \"\"\"\n",
    "        Handles a query by retrieving relevant information from the knowledge graph and visualizing the traversal path.\n",
    "        \n",
    "        Args:\n",
    "        - query (str): The query to be answered.\n",
    "        \n",
    "        Returns:\n",
    "        - str: The response to the query.\n",
    "        \"\"\"\n",
    "        response, traversal_path, filtered_content = self.query_engine.query(query)\n",
    "        \n",
    "        if traversal_path:\n",
    "            self.visualizer.visualize_traversal(self.knowledge_graph.graph, traversal_path)\n",
    "        else:\n",
    "            print(\"No traversal path to visualize.\")\n",
    "        \n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define documents path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T11:34:41.032190Z",
     "start_time": "2025-02-20T11:34:41.025820Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"../data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T11:34:42.812249Z",
     "start_time": "2025-02-20T11:34:41.062344Z"
    }
   },
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(path)\n",
    "documents_pdf = loader.load()\n",
    "documents_pdf = documents_pdf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T11:44:49.614082Z",
     "start_time": "2025-02-20T11:44:49.576947Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import JSONLoader\n",
    "\n",
    "# Percorso del file JSON\n",
    "json_path = \"../data/Result_big.json\"\n",
    "\n",
    "# Crea il loader e carica i dati\n",
    "loader = JSONLoader(file_path=json_path, jq_schema=\".\", text_content=False)  # Usa jq_schema per specificare come estrarre i dati\n",
    "documents_json = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "# Percorso del file Markdown\n",
    "md_path = \"../data/Referto_varchar.md\"\n",
    "\n",
    "# Crea il loader e carica i dati\n",
    "loader = UnstructuredMarkdownLoader(md_path)\n",
    "documents_md = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a graph RAG instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rag = GraphRAG()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the documents and create the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting concepts and entities: 100%|| 9234/9235 [46:44<00:00,  2.39it/s]    "
     ]
    }
   ],
   "source": [
    "graph_rag.process_documents(documents_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input a query and get the retrieved information from the graph RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'query'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrovami tutti i casi con il carcinoma\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 2\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mgraph_rag\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[7], line 46\u001B[0m, in \u001B[0;36mGraphRAG.query\u001B[1;34m(self, query)\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mquery\u001B[39m(\u001B[38;5;28mself\u001B[39m, query: \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m     37\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;124;03m    Handles a query by retrieving relevant information from the knowledge graph and visualizing the traversal path.\u001B[39;00m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;124;03m    \u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;124;03m    - str: The response to the query.\u001B[39;00m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 46\u001B[0m     response, traversal_path, filtered_content \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery\u001B[49m(query)\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m traversal_path:\n\u001B[0;32m     49\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvisualizer\u001B[38;5;241m.\u001B[39mvisualize_traversal(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mknowledge_graph\u001B[38;5;241m.\u001B[39mgraph, traversal_path)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'query'"
     ]
    }
   ],
   "source": [
    "query = \"Trovami tutti i casi con il carcinoma\"\n",
    "response = graph_rag.query(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
